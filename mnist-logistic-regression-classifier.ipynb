{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying MNIST digits using Logistic Regression\n",
    "\n",
    "This notebook closely follows this tutorial: http://deeplearning.net/tutorial/logreg.html\n",
    "\n",
    "The MNIST digits dataset is separated in training, verification and test subsets. A logistic regression classifier is trained with the following model:\n",
    "```\n",
    "prediction = argmax_i(softmax_i(Wx + b))\n",
    "```\n",
    "where\n",
    "* `prediction` is the predicted class (digit) for the input `x`\n",
    "* `W` is a weight matrix\n",
    "* `b` is a bias vector\n",
    "* `i` is one of the possible classes (digits)\n",
    "The classifier uses the negative log likelihood (NLL) loss function:\n",
    "```\n",
    "l = -log(P(Y=y(i)|x(i),W,b))\n",
    "```\n",
    "where\n",
    "* `x(i)` is an input vector that should be classified as class `i`\n",
    "* `y(i)` is a prediction for class `i`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful flags:\n",
    "```\n",
    "THEANO_FLAGS=floatX=float64,device=cuda,traceback.limit=20\n",
    "```\n",
    "use `device=cuda` or `device=cpu`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "import os\n",
    "import timeit\n",
    "\n",
    "import numpy\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some global constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The MNIST dataset - loaded in suitable python format and pickled (see the tutorial)\n",
    "DATASET = \"E:/ml/datasets/mnist.pkl.gz\"\n",
    "\n",
    "# The save location for the best trained model\n",
    "SAVED_MODEL = \"E:/ml/mnist/my_best_model.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of a helper function that reads a dataset from gziped pickle file and loads it into shared Theano variables. The idea is to have the whole dataset copied into the GPU's memory in a single operation rather than copying it in chucks on each training step. The slices of the data that are needed for a given operation (training/verification/test step) are accessed via indices later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset):\n",
    "    data_dir, data_file = os.path.split(dataset)\n",
    "    if data_dir == \"\" and not os.path.isfile(dataset):\n",
    "        raise ValueError('Incorrect dataset path', ('dataset', dataset))\n",
    "    print('... loading data')\n",
    "    with gzip.open(dataset, 'rb') as f:\n",
    "            train_set, validate_set, test_set = pickle.load(f, encoding='latin1')\n",
    "    def shared_dataset(data_xy, borrow=True):\n",
    "        data_x, data_y = data_xy\n",
    "        shared_x = theano.shared(\n",
    "            numpy.asarray(data_x, dtype=theano.config.floatX),\n",
    "            borrow=borrow)\n",
    "        # since we intend to keep this in the GPU's memory, we need to store it as a float and not an int\n",
    "        shared_y = theano.shared(\n",
    "            numpy.asarray(data_y, dtype=theano.config.floatX),\n",
    "            borrow=borrow)\n",
    "        # however, we cast it to int so we don't have to manage float pointing numbers when indexing the prediction classes\n",
    "        return shared_x, T.cast(shared_y, 'int32')\n",
    "\n",
    "    test_set_x, test_set_y = shared_dataset(test_set)\n",
    "    validate_set_x, validate_set_y = shared_dataset(validate_set)\n",
    "    train_set_x, train_set_y = shared_dataset(train_set)\n",
    "\n",
    "    rval = [(train_set_x, train_set_y),\n",
    "            (validate_set_x, validate_set_y),\n",
    "            (test_set_x, test_set_y)]\n",
    "    return rval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of a LogisticRegression class. This will hold all the parameters (W and b) of our classifier. It also defines the loss function (negative_log_likelihood) for the classifier as well as a helper zero-one loss function (errors) that is used to validate the occuracy of the model during training. A trained model can be pickled so it can be later loaded and used out of the box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(object):\n",
    "    def __init__(self, x, n_in, n_out):\n",
    "        \"\"\"\n",
    "        x: tensor, the input data for this LogisticRegression. This is a (micro)batch of one or more datapoints from th dataset\n",
    "        n_in: integer, number of inputs (image width * height)\n",
    "        n_out: integer, number of outputs (classes - 10 for digits)\n",
    "        \"\"\"\n",
    "        \n",
    "        # W and b are shared so the progress between iterations can be preserved\n",
    "        self.W = theano.shared(\n",
    "            value=numpy.zeros(\n",
    "                (n_in, n_out),\n",
    "                dtype=theano.config.floatX\n",
    "            ),\n",
    "            name='W',\n",
    "            borrow=True\n",
    "        )\n",
    "        self.b = theano.shared(\n",
    "            value=numpy.zeros(\n",
    "                (n_out,),\n",
    "                dtype=theano.config.floatX\n",
    "            ),\n",
    "            name='b',\n",
    "            borrow=True\n",
    "        )\n",
    "        \n",
    "        # define the model of the classifier\n",
    "        self.p_y_given_x = T.nnet.softmax(T.dot(x, self.W) + self.b)\n",
    "        self.y_pred = T.argmax(self.p_y_given_x, axis=1)\n",
    "        \n",
    "        self.params = [self.W, self.b]\n",
    "        self.x = x\n",
    "        \n",
    "    # define the loss function of the model   \n",
    "    def negative_log_likelihood(self, y):\n",
    "        return -T.mean(T.log(self.p_y_given_x)[T.arange(y.shape[0]), y])\n",
    "    \n",
    "    # define a helper zero-one loss function for the model that can be used during testing/validation of the model\n",
    "    def errors(self, y):\n",
    "        return T.mean(T.neq(self.y_pred, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the heart of the algorithm - the microbatsches stohastic gradient descend (MSGD). Here the data is loaded and partitioned in microbatches. On each training step of the classifier, one whole batch of data is used. After all batches are used, we start all over and we count the next epoch. Classifier params are persisted between steps and epochs.\n",
    "\n",
    "To try to prevent overfitting, an early exit will be triggered in the training algorithm in case the occuracy of the classifier does not improve in a number of steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def msgd_optimization_mnist(learning_rate=0.13,\n",
    "                           n_epochs=1000,\n",
    "                           dataset=DATASET,\n",
    "                           batch_size=600):\n",
    "    # load the dataset and split it in sets\n",
    "    datasets = load_data(dataset)\n",
    "    train_set_x, train_set_y = datasets[0]\n",
    "    validate_set_x, validate_set_y = datasets[1]\n",
    "    test_set_x, test_set_y = datasets[2]\n",
    "\n",
    "    # Calculate the sizes of the batches\n",
    "    n_train_batches = train_set_x.get_value(borrow=True).shape[0] // batch_size\n",
    "    n_validate_batches = validate_set_x.get_value(borrow=True).shape[0] // batch_size\n",
    "    n_test_batches = test_set_x.get_value(borrow=True).shape[0] // batch_size\n",
    "    \n",
    "    #############\n",
    "    # Build model\n",
    "    #############\n",
    "    print('... building the model')\n",
    "    \n",
    "    # everything in this section is interpreted as symbolic assignments. Nothing is evaluated until a function is called\n",
    "    \n",
    "    index = T.lscalar() # the training batch index\n",
    "    x = T.matrix('x') # the input. It is a matrix because multiple input vectors are processed at the same time\n",
    "    y = T.ivector('y') # the labels for each input vector\n",
    "    \n",
    "    # instantiate the classifier and define a cost equal to the loss\n",
    "    classifier = LogisticRegression(x=x, n_in=28*28, n_out=10)\n",
    "    cost = classifier.negative_log_likelihood(y)\n",
    "    \n",
    "    # define the gradients of the classifier's parameters\n",
    "    grad_W = T.grad(cost=cost, wrt=classifier.W)\n",
    "    grad_b = T.grad(cost=cost, wrt=classifier.b)\n",
    "    \n",
    "    # gradient descent - on each step we are going to update W and b in proportion to their corresponding gradients\n",
    "    updates = [\n",
    "        (classifier.W, classifier.W - learning_rate * grad_W),\n",
    "        (classifier.b, classifier.b - learning_rate * grad_b)\n",
    "    ]\n",
    "    \n",
    "    # a helpper function that will be used to feed given batches of the datasets to a function\n",
    "    def givens(dataset_x, dataset_y):\n",
    "        return {\n",
    "            x: dataset_x[index * batch_size : (index + 1) * batch_size],\n",
    "            y: dataset_y[index * batch_size : (index + 1) * batch_size]\n",
    "        }\n",
    "    \n",
    "    # Define the function that performs a training step\n",
    "    train_model = theano.function(\n",
    "        inputs=[index], # takes the batch index as input\n",
    "        outputs=cost, # returns the cost as output\n",
    "        updates=updates, # updates the model on each step\n",
    "        givens=givens(train_set_x, train_set_y) # takes batches from the training sets\n",
    "    )\n",
    "    # Define a function that validates the occurracy of the model during training\n",
    "    validate_model = theano.function(\n",
    "        inputs=[index], # takes a batch index as input\n",
    "        outputs=classifier.errors(y), # outputs the ratio of wrong predictions\n",
    "        givens=givens(validate_set_x, validate_set_y) # takes batches from the validation sets\n",
    "    )\n",
    "    # Define a function that tests an already trained model\n",
    "    test_model = theano.function(\n",
    "        inputs=[index], # takes a batch index as input\n",
    "        outputs=classifier.errors(y), # outputs the ratio of wrong predictions\n",
    "        givens=givens(test_set_x, test_set_y) # takes batches from the training sets\n",
    "    )\n",
    "    \n",
    "    #############\n",
    "    # Train model\n",
    "    #############\n",
    "    print('... training the model')\n",
    "    \n",
    "    # In this section the model is actually trained, validated and tested.\n",
    "    \n",
    "    patience = 5000 # for early stopping, how many iterations to take at maximum\n",
    "    patience_mult = 2 # when new best model is found, this much more iterations will be trained\n",
    "    improvement_thresh = 0.995 # defines what is a \"better model\"\n",
    "    validation_freq = min(n_train_batches, patience // 2) # number of iterations between validations\n",
    "\n",
    "    best_validation_loss = numpy.inf\n",
    "    test_score = 0.\n",
    "\n",
    "    done_looping = False # for early stopping\n",
    "    epoch = 0\n",
    "    start_time = timeit.default_timer()\n",
    "    \n",
    "    while (epoch < n_epochs) and (not done_looping):\n",
    "        epoch = epoch + 1\n",
    "        for minibatch_index in range(n_train_batches):\n",
    "            # train\n",
    "            minibatch_avg_cost = train_model(minibatch_index)\n",
    "            \n",
    "            # do validation with all validation samples\n",
    "            iter_no = (epoch - 1) * n_train_batches + minibatch_index\n",
    "            if (iter_no + 1) % validation_freq == 0:\n",
    "                # calculate the mean loss over all validation samples\n",
    "                validation_loss = numpy.mean(\n",
    "                    [validate_model(i) for i in range(n_validate_batches)]\n",
    "                )\n",
    "                print(\n",
    "                    'epoch %i, minibatch %i/%i, validation error %f %%' %\n",
    "                    (\n",
    "                        epoch,\n",
    "                        minibatch_index + 1,\n",
    "                        n_train_batches,\n",
    "                        validation_loss * 100.\n",
    "                    )\n",
    "                )\n",
    "                \n",
    "                if validation_loss < best_validation_loss:\n",
    "                    # increase patience\n",
    "                    if validation_loss < improvement_thresh * best_validation_loss:\n",
    "                        patience = max(patience, iter_no * patience_mult)\n",
    "                    best_validation_loss = validation_loss\n",
    "                    \n",
    "                    # calculate test score using all test samples\n",
    "                    test_score = 1. - numpy.mean(\n",
    "                        [test_model(i) for i in range(n_test_batches)]\n",
    "                    )\n",
    "                    print(\n",
    "                        ' * epoch %i, minibatch %i/%i, test error of best model %f %%' %\n",
    "                        (\n",
    "                            epoch,\n",
    "                            minibatch_index + 1,\n",
    "                            n_train_batches,\n",
    "                            test_score * 100.\n",
    "                        )\n",
    "                    )\n",
    "                    \n",
    "                    # pickle the best model for later use\n",
    "                    with open(SAVED_MODEL, 'wb') as f:\n",
    "                        pickle.dump(classifier, f)\n",
    "            \n",
    "            # early exit\n",
    "            if iter_no >= patience:\n",
    "                done_looping = True\n",
    "                break\n",
    "    \n",
    "    end_time = timeit.default_timer()\n",
    "    print(\n",
    "        'Optimization complete with best validation score of %f %%, with test performance %f %%'\n",
    "        % (\n",
    "            best_validation_loss * 100.,\n",
    "            test_score * 100.\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        'The code run for %d epochs, with %f epochs/sec'\n",
    "        % (\n",
    "            epoch,\n",
    "            1. * epoch / (end_time - start_time)\n",
    "        )\n",
    "    )\n",
    "    print(('The notebook ran for %.1fs' % ((end_time - start_time))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A helper function that can use a pre-trained model to predict the classes for some input samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(saved_model, x):\n",
    "    \"\"\"\n",
    "    saved_model: string, location of the saved model\n",
    "    x: tensor, input vectors to be classified concatenated into a tensor\n",
    "    \"\"\"\n",
    "    with open(saved_model, 'rb') as f:\n",
    "        classifier = pickle.load(f)\n",
    "    \n",
    "    prediction = theano.function(\n",
    "        inputs=[classifier.x],\n",
    "        outputs=classifier.y_pred\n",
    "    )\n",
    "    \n",
    "    return prediction(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main entrypoint\n",
    "\n",
    "This trains the model, saves the trained classifier and tests it with the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loading data\n",
      "... building the model\n",
      "... training the model\n",
      "epoch 1, minibatch 83/83, validation error 12.458333 %\n",
      " * epoch 1, minibatch 83/83, test error of best model 87.625000 %\n",
      "epoch 2, minibatch 83/83, validation error 11.010417 %\n",
      " * epoch 2, minibatch 83/83, test error of best model 89.041667 %\n",
      "epoch 3, minibatch 83/83, validation error 10.312500 %\n",
      " * epoch 3, minibatch 83/83, test error of best model 89.687500 %\n",
      "epoch 4, minibatch 83/83, validation error 9.875000 %\n",
      " * epoch 4, minibatch 83/83, test error of best model 90.166667 %\n",
      "epoch 5, minibatch 83/83, validation error 9.562500 %\n",
      " * epoch 5, minibatch 83/83, test error of best model 90.520833 %\n",
      "epoch 6, minibatch 83/83, validation error 9.322917 %\n",
      " * epoch 6, minibatch 83/83, test error of best model 90.708333 %\n",
      "epoch 7, minibatch 83/83, validation error 9.187500 %\n",
      " * epoch 7, minibatch 83/83, test error of best model 91.000000 %\n",
      "epoch 8, minibatch 83/83, validation error 8.989583 %\n",
      " * epoch 8, minibatch 83/83, test error of best model 91.041667 %\n",
      "epoch 9, minibatch 83/83, validation error 8.937500 %\n",
      " * epoch 9, minibatch 83/83, test error of best model 91.187500 %\n",
      "epoch 10, minibatch 83/83, validation error 8.750000 %\n",
      " * epoch 10, minibatch 83/83, test error of best model 91.333333 %\n",
      "epoch 11, minibatch 83/83, validation error 8.666667 %\n",
      " * epoch 11, minibatch 83/83, test error of best model 91.479167 %\n",
      "epoch 12, minibatch 83/83, validation error 8.583333 %\n",
      " * epoch 12, minibatch 83/83, test error of best model 91.583333 %\n",
      "epoch 13, minibatch 83/83, validation error 8.489583 %\n",
      " * epoch 13, minibatch 83/83, test error of best model 91.708333 %\n",
      "epoch 14, minibatch 83/83, validation error 8.427083 %\n",
      " * epoch 14, minibatch 83/83, test error of best model 91.718750 %\n",
      "epoch 15, minibatch 83/83, validation error 8.354167 %\n",
      " * epoch 15, minibatch 83/83, test error of best model 91.729167 %\n",
      "epoch 16, minibatch 83/83, validation error 8.302083 %\n",
      " * epoch 16, minibatch 83/83, test error of best model 91.760417 %\n",
      "epoch 17, minibatch 83/83, validation error 8.250000 %\n",
      " * epoch 17, minibatch 83/83, test error of best model 91.822917 %\n",
      "epoch 18, minibatch 83/83, validation error 8.229167 %\n",
      " * epoch 18, minibatch 83/83, test error of best model 91.937500 %\n",
      "epoch 19, minibatch 83/83, validation error 8.260417 %\n",
      "epoch 20, minibatch 83/83, validation error 8.260417 %\n",
      "epoch 21, minibatch 83/83, validation error 8.208333 %\n",
      " * epoch 21, minibatch 83/83, test error of best model 92.052083 %\n",
      "epoch 22, minibatch 83/83, validation error 8.187500 %\n",
      " * epoch 22, minibatch 83/83, test error of best model 92.072917 %\n",
      "epoch 23, minibatch 83/83, validation error 8.156250 %\n",
      " * epoch 23, minibatch 83/83, test error of best model 92.041667 %\n",
      "epoch 24, minibatch 83/83, validation error 8.114583 %\n",
      " * epoch 24, minibatch 83/83, test error of best model 92.052083 %\n",
      "epoch 25, minibatch 83/83, validation error 8.093750 %\n",
      " * epoch 25, minibatch 83/83, test error of best model 92.052083 %\n",
      "epoch 26, minibatch 83/83, validation error 8.104167 %\n",
      "epoch 27, minibatch 83/83, validation error 8.104167 %\n",
      "epoch 28, minibatch 83/83, validation error 8.052083 %\n",
      " * epoch 28, minibatch 83/83, test error of best model 92.156250 %\n",
      "epoch 29, minibatch 83/83, validation error 8.052083 %\n",
      "epoch 30, minibatch 83/83, validation error 8.031250 %\n",
      " * epoch 30, minibatch 83/83, test error of best model 92.156250 %\n",
      "epoch 31, minibatch 83/83, validation error 8.010417 %\n",
      " * epoch 31, minibatch 83/83, test error of best model 92.166667 %\n",
      "epoch 32, minibatch 83/83, validation error 7.979167 %\n",
      " * epoch 32, minibatch 83/83, test error of best model 92.187500 %\n",
      "epoch 33, minibatch 83/83, validation error 7.947917 %\n",
      " * epoch 33, minibatch 83/83, test error of best model 92.260417 %\n",
      "epoch 34, minibatch 83/83, validation error 7.875000 %\n",
      " * epoch 34, minibatch 83/83, test error of best model 92.270833 %\n",
      "epoch 35, minibatch 83/83, validation error 7.885417 %\n",
      "epoch 36, minibatch 83/83, validation error 7.843750 %\n",
      " * epoch 36, minibatch 83/83, test error of best model 92.302083 %\n",
      "epoch 37, minibatch 83/83, validation error 7.802083 %\n",
      " * epoch 37, minibatch 83/83, test error of best model 92.364583 %\n",
      "epoch 38, minibatch 83/83, validation error 7.812500 %\n",
      "epoch 39, minibatch 83/83, validation error 7.812500 %\n",
      "epoch 40, minibatch 83/83, validation error 7.822917 %\n",
      "epoch 41, minibatch 83/83, validation error 7.791667 %\n",
      " * epoch 41, minibatch 83/83, test error of best model 92.375000 %\n",
      "epoch 42, minibatch 83/83, validation error 7.770833 %\n",
      " * epoch 42, minibatch 83/83, test error of best model 92.385417 %\n",
      "epoch 43, minibatch 83/83, validation error 7.750000 %\n",
      " * epoch 43, minibatch 83/83, test error of best model 92.406250 %\n",
      "epoch 44, minibatch 83/83, validation error 7.739583 %\n",
      " * epoch 44, minibatch 83/83, test error of best model 92.406250 %\n",
      "epoch 45, minibatch 83/83, validation error 7.739583 %\n",
      "epoch 46, minibatch 83/83, validation error 7.739583 %\n",
      "epoch 47, minibatch 83/83, validation error 7.739583 %\n",
      "epoch 48, minibatch 83/83, validation error 7.708333 %\n",
      " * epoch 48, minibatch 83/83, test error of best model 92.416667 %\n",
      "epoch 49, minibatch 83/83, validation error 7.677083 %\n",
      " * epoch 49, minibatch 83/83, test error of best model 92.427083 %\n",
      "epoch 50, minibatch 83/83, validation error 7.677083 %\n",
      "epoch 51, minibatch 83/83, validation error 7.677083 %\n",
      "epoch 52, minibatch 83/83, validation error 7.656250 %\n",
      " * epoch 52, minibatch 83/83, test error of best model 92.458333 %\n",
      "epoch 53, minibatch 83/83, validation error 7.656250 %\n",
      "epoch 54, minibatch 83/83, validation error 7.635417 %\n",
      " * epoch 54, minibatch 83/83, test error of best model 92.479167 %\n",
      "epoch 55, minibatch 83/83, validation error 7.635417 %\n",
      "epoch 56, minibatch 83/83, validation error 7.635417 %\n",
      "epoch 57, minibatch 83/83, validation error 7.604167 %\n",
      " * epoch 57, minibatch 83/83, test error of best model 92.510417 %\n",
      "epoch 58, minibatch 83/83, validation error 7.583333 %\n",
      " * epoch 58, minibatch 83/83, test error of best model 92.541667 %\n",
      "epoch 59, minibatch 83/83, validation error 7.572917 %\n",
      " * epoch 59, minibatch 83/83, test error of best model 92.531250 %\n",
      "epoch 60, minibatch 83/83, validation error 7.572917 %\n",
      "epoch 61, minibatch 83/83, validation error 7.583333 %\n",
      "epoch 62, minibatch 83/83, validation error 7.572917 %\n",
      " * epoch 62, minibatch 83/83, test error of best model 92.479167 %\n",
      "epoch 63, minibatch 83/83, validation error 7.562500 %\n",
      " * epoch 63, minibatch 83/83, test error of best model 92.489583 %\n",
      "epoch 64, minibatch 83/83, validation error 7.572917 %\n",
      "epoch 65, minibatch 83/83, validation error 7.562500 %\n",
      "epoch 66, minibatch 83/83, validation error 7.552083 %\n",
      " * epoch 66, minibatch 83/83, test error of best model 92.479167 %\n",
      "epoch 67, minibatch 83/83, validation error 7.552083 %\n",
      "epoch 68, minibatch 83/83, validation error 7.531250 %\n",
      " * epoch 68, minibatch 83/83, test error of best model 92.479167 %\n",
      "epoch 69, minibatch 83/83, validation error 7.531250 %\n",
      "epoch 70, minibatch 83/83, validation error 7.510417 %\n",
      " * epoch 70, minibatch 83/83, test error of best model 92.500000 %\n",
      "epoch 71, minibatch 83/83, validation error 7.520833 %\n",
      "epoch 72, minibatch 83/83, validation error 7.510417 %\n",
      "epoch 73, minibatch 83/83, validation error 7.500000 %\n",
      " * epoch 73, minibatch 83/83, test error of best model 92.510417 %\n",
      "Optimization complete with best validation score of 7.500000 %, with test performance 92.510417 %\n",
      "The code run for 74 epochs, with 7.385765 epochs/sec\n",
      "The notebook ran for 10.0s\n"
     ]
    }
   ],
   "source": [
    "msgd_optimization_mnist(dataset=DATASET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained entrypoint\n",
    "\n",
    "This uses a pre-trained model to predict the classes of the test samples from a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loading data\n",
      "Prediction for first 10 samples:\n",
      "[7 2 1 0 4 1 4 9 6 9]\n",
      "Labels for first 10 samples:\n",
      "[7 2 1 0 4 1 4 9 5 9]\n"
     ]
    }
   ],
   "source": [
    "datasets = load_data(DATASET)\n",
    "test_set_x, test_set_y = datasets[2]\n",
    "\n",
    "get_labels = theano.function(\n",
    "    inputs=[],\n",
    "    outputs=test_set_y\n",
    ")\n",
    "\n",
    "k = 10\n",
    "prediction = predict(SAVED_MODEL, test_set_x.get_value()[:k])\n",
    "labels = get_labels()[:k]\n",
    "\n",
    "print(\"Prediction for first %i samples:\" % (k))\n",
    "print(prediction)\n",
    "print(\"Labels for first %i samples:\" % (k))\n",
    "print(labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
